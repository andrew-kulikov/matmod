{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loop: 0\n",
      "\n",
      "Next request at loop: -1\n",
      "\n",
      "\n",
      "\n",
      "All done: 0\n",
      "\n",
      "All accepted: 0\n",
      "\n",
      "All rejected: 0\n",
      "\n",
      "All cancelled: 0\n",
      "\n",
      "\n",
      "\n",
      "Messages (0):\n",
      "\n",
      "\n",
      "Process:\n",
      "\n",
      "0: None. Done at loop: -1\n",
      "\n",
      "1: None. Done at loop: -1\n",
      "\n",
      "State 0: 0.014001400140014001 - 0.0028338522898226216, 0.011167547850191379\n",
      "State 1: 0.041904190419041906 - 0.018892348598817478, 0.023011841820224428\n",
      "State 2: 0.022602260226022602 - 0.06297449532939159, 0.040372235103368986\n",
      "State 3: 0.1222122212221222 - 0.02623937305391317, 0.09597284816820903\n",
      "State 4: 0.3852385238523852 - 0.0437322884231886, 0.3415062354291966\n",
      "State 5: 0.414041404140414 - 0.021866144211594305, 0.3921752599288197\n",
      "Max diff: 0.3921752599288197\n",
      "\n",
      "Relative bandwidth: 0.585958595859586 - 0.9781338557884057, 0.39217525992881974\n",
      "Absolute bandwidth: 0.00585958595859586 - 0.009781338557884057, 0.003921752599288197\n",
      "\n",
      "Cancellations: 0.6947368421052632 - 0.052478746107826324, 0.6422580959974369\n",
      "\n",
      "Average waiting queue length: 2.134813481348135 - 0.1793023825350733, 1.9555110988130617\n",
      "\n",
      "Average working channels count: 1.93009300930093 - 0.3285169506349928, 1.6015760586659373\n",
      "\n",
      "Average requests count in system: 4.064906490649065 - 0.5078193331700661, 3.557087157478999\n",
      "\n",
      "Average requests live time in system: 406.4906490649065 - 50.78193331700661, 355.7087157478999\n",
      "\n",
      "Average requests live time in queue: 213.4813481348135 - 17.93023825350733, 195.55110988130616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"float\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8d4a7f448724>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m \u001b[0mrun_live_time_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[0mshow_stationary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8d4a7f448724>\u001b[0m in \u001b[0;36mrun_live_time_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_array\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0my_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_print\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msystem_live_time_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8d4a7f448724>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(loop_limit, to_print, finish_state_statistic_only, system_live_time_only)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[0maverage_waiting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_average_waiting_queue_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_print\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[0maverage_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_average_working_channels_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_print\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0maverage_system\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_system_pract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_average_requests_in_system_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_print\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_waiting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmake_average_request_live_in_system\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_print\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_system\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_system_pract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8d4a7f448724>\u001b[0m in \u001b[0;36mmake_average_requests_in_system_count\u001b[1;34m(to_print, average_waiting, average_channels)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_average_requests_in_system_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_print\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_waiting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[0mpract_average\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests_count_in_system_in_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m     \u001b[0mtheor_average\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_waiting\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maverage_channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto_print\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"float\") to tuple"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "\n",
    "import sympy as sp\n",
    "from sympy.solvers.solveset import linsolve\n",
    "\n",
    "waiting_queue = []\n",
    "channels = []\n",
    "all_accepted = 0\n",
    "all_rejected = 0\n",
    "all_done = 0\n",
    "all_requests = 0\n",
    "all_cancelled = 0\n",
    "\n",
    "current_loop = 0\n",
    "next_message_loop = -1\n",
    "\n",
    "\n",
    "last_value = 0\n",
    "\n",
    "\n",
    "def print_system_state():\n",
    "    print(\"Current loop: {}\\n\".format(current_loop))\n",
    "    print(\"Next request at loop: {}\\n\".format(next_message_loop))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"All done: {}\\n\".format(all_done))\n",
    "    print(\"All accepted: {}\\n\".format(all_accepted))\n",
    "    print(\"All rejected: {}\\n\".format(all_rejected))\n",
    "    print(\"All cancelled: {}\\n\".format(all_cancelled))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Messages ({}):\\n\".format(len(waiting_queue)))\n",
    "    for val in list(waiting_queue).__reversed__():\n",
    "        print(\"{} \".format(val))\n",
    "\n",
    "    print(\"\\nProcess:\\n\")\n",
    "    for channel in channels:\n",
    "        print(\"{}: {}. Done at loop: {}\\n\".format(channel.number, channel.value, channel.resume_on_loop))\n",
    "\n",
    "def generate_next_exponential(lamb):\n",
    "    bsv = np.random.uniform()\n",
    "    return - 1 / lamb * np.log(1 - bsv)\n",
    "\n",
    "\n",
    "class Channel():\n",
    "\n",
    "    def __init__(self, number, on_accepted, on_rejected, on_done):\n",
    "        self.number = number\n",
    "        self.on_accepted = on_accepted\n",
    "        self.on_rejected = on_rejected\n",
    "        self.on_done = on_done\n",
    "        self.accept_prob = accept_prob\n",
    "        self.intensity = channel_intensity\n",
    "\n",
    "        self.working = False\n",
    "        self.resume_on_loop = -1\n",
    "        self.value = None\n",
    "\n",
    "    def should_accept(self, p):\n",
    "        return np.random.choice([True, False], p=[p, 1 - p])\n",
    "\n",
    "    def set_data(self, val):\n",
    "        dist = generate_next_exponential(self.intensity)\n",
    "        self.resume_on_loop = current_loop + dist\n",
    "        self.working = True\n",
    "        self.value = val\n",
    "\n",
    "    def on_new_loop(self):\n",
    "        if self.resume_on_loop != -1 and current_loop >= self.resume_on_loop:\n",
    "            self.working = False\n",
    "            self.resume_on_loop = -1\n",
    "            val = self.value\n",
    "            self.value = None\n",
    "\n",
    "            accept = self.should_accept(self.accept_prob)\n",
    "            if accept:\n",
    "                self.on_accepted(self.number, val)\n",
    "            else:\n",
    "                self.on_rejected(self.number, val)\n",
    "\n",
    "            self.on_done(self.number, val)\n",
    "\n",
    "\n",
    "def on_channel_accept(number, val):\n",
    "    global all_accepted\n",
    "    all_accepted += 1\n",
    "\n",
    "    request_live_end[val] = current_loop\n",
    "\n",
    "def on_channel_reject(number, val):\n",
    "    global all_rejected, all_cancelled\n",
    "    all_rejected += 1\n",
    "\n",
    "    rejected_count[val] = rejected_count.get(val, 0) + 1\n",
    "    put_in_waiting_queue(val)\n",
    "\n",
    "def on_channel_done(number, val):\n",
    "    global all_done, waiting_queue\n",
    "    all_done += 1\n",
    "\n",
    "    refresh_channels()\n",
    "\n",
    "def put_in_waiting_queue(val):\n",
    "    global waiting_queue, all_cancelled\n",
    "    if len(waiting_queue) == waiting_queue_limit:\n",
    "        all_cancelled += 1\n",
    "        if request_live_start.get(val, None) is not None:\n",
    "            request_live_end[val] = current_loop\n",
    "        return\n",
    "\n",
    "    if request_live_start.get(val, None) is None:\n",
    "        request_live_start[val] = current_loop\n",
    "\n",
    "    request_in_queue_start[val] = current_loop + request_in_queue_start.get(val, 0)\n",
    "\n",
    "    waiting_queue.append(val)\n",
    "    return\n",
    "\n",
    "\n",
    "def start_channels():\n",
    "    running_channels = []\n",
    "\n",
    "    for i in range(channel_count):\n",
    "        channel = Channel(i, on_channel_accept, on_channel_reject, on_channel_done)\n",
    "        running_channels.append(channel)\n",
    "\n",
    "    return running_channels\n",
    "\n",
    "\n",
    "def refresh_channels():\n",
    "    global channels, waiting_queue\n",
    "\n",
    "    for channel in channels:\n",
    "        channel.on_new_loop()\n",
    "        if not channel.working and len(waiting_queue) != 0:\n",
    "            first = waiting_queue[0]\n",
    "            waiting_queue = waiting_queue[1:]\n",
    "            channel.set_data(first)\n",
    "            request_in_queue_end[first] = current_loop + request_in_queue_end.get(first, 0)\n",
    "\n",
    "def try_create_new_request():\n",
    "    global current_loop, next_message_loop, last_value, all_requests\n",
    "\n",
    "    if current_loop >= next_message_loop:\n",
    "        last_value += 1\n",
    "        all_requests += 1\n",
    "        put_in_waiting_queue(last_value)\n",
    "\n",
    "        diff = generate_next_exponential(request_intensity)\n",
    "        next_message_loop = current_loop + diff\n",
    "\n",
    "\n",
    "finish_state_count = []\n",
    "finish_state_prob_in_time = []\n",
    "requests_count_in_system_in_time = []\n",
    "waiting_queue_len_in_time = []\n",
    "working_channels_count_in_time = []\n",
    "request_live_start = {}\n",
    "request_live_end = {}\n",
    "request_in_queue_start = {}\n",
    "request_in_queue_end = {}\n",
    "rejected_count = {}\n",
    "unique_requests_in_time = []\n",
    "accepted_count_in_time = []\n",
    "prev_accepted = 0\n",
    "\n",
    "def init_statistic():\n",
    "    global waiting_queue_limit, finish_state_count, channel_count\n",
    "\n",
    "    finish_state_count = [0 for i in range(channel_count + waiting_queue_limit + 1)]\n",
    "\n",
    "\n",
    "def take_statistic():\n",
    "    global finish_state_count, channels, waiting_queue, prev_accepted\n",
    "\n",
    "    requests = 0\n",
    "    working_channels_count = 0\n",
    "    for channel in channels:\n",
    "        if channel.working:\n",
    "            requests += 1\n",
    "\n",
    "    working_channels_count = requests\n",
    "    requests += len(waiting_queue)\n",
    "\n",
    "    finish_state_count[requests] += 1\n",
    "\n",
    "    finish_state_prob = []\n",
    "    for c in finish_state_count:\n",
    "        finish_state_prob.append(c / sum(finish_state_count))\n",
    "    finish_state_prob_in_time.append(finish_state_prob)\n",
    "\n",
    "    requests_count_in_system_in_time.append(requests)\n",
    "    waiting_queue_len_in_time.append(len(waiting_queue))\n",
    "    working_channels_count_in_time.append(working_channels_count)\n",
    "\n",
    "    unique_requests = 0\n",
    "    for channel in channels:\n",
    "        if rejected_count.get(channel.value, 0) == 0:\n",
    "            unique_requests += 1\n",
    "    for val in waiting_queue:\n",
    "        if rejected_count.get(val, 0) == 0:\n",
    "            unique_requests += 1\n",
    "    unique_requests_in_time.append(unique_requests)\n",
    "\n",
    "    accepted_count_in_time.append(all_accepted)\n",
    "\n",
    "def make_finish_state_statistic(to_print):\n",
    "    global finish_state_count, channel_count\n",
    "\n",
    "    s = sum(finish_state_count)\n",
    "    max_diff = 0\n",
    "    practice_finish_state = []\n",
    "    theor_finish_state = []\n",
    "    for i in range(len(finish_state_count)):\n",
    "        pract = finish_state_count[i] / s\n",
    "        theor = get_finish_n_state_statistic(i)\n",
    "        diff = np.abs(pract - theor)\n",
    "\n",
    "        practice_finish_state.append(pract)\n",
    "        theor_finish_state.append(theor)\n",
    "\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "\n",
    "        if to_print:\n",
    "            print(\"State {}: {} - {}, {}\".format(i, pract, theor, diff))\n",
    "\n",
    "    if to_print:\n",
    "        print(\"Max diff: {}\".format(max_diff))\n",
    "\n",
    "    return practice_finish_state, theor_finish_state, max_diff\n",
    "\n",
    "def get_finish_n_state_statistic(i):\n",
    "    return theor_state_prob[i]\n",
    "\n",
    "    # changed_channel_intensity = channel_intensity * accept_prob\n",
    "    # if i != channel_count + waiting_queue_limit:\n",
    "    #     changed_channel_intensity *= accept_prob\n",
    "    #\n",
    "    # if i != channel_count * waiting_queue_limit - 1:\n",
    "    #     changed_channel_intensity =\n",
    "    #\n",
    "    # if i > channel_count:\n",
    "    #     theor = request_intensity ** i / (np.math.factorial(channel_count) * channel_count ** (i - channel_count) * changed_channel_intensity ** i)\n",
    "    # else:\n",
    "    #     theor = request_intensity ** i / (np.math.factorial(i) * changed_channel_intensity ** i)\n",
    "    #\n",
    "    # f = 0\n",
    "    # for j in range(channel_count + 1):\n",
    "    #     f += request_intensity ** j / (np.math.factorial(j) * changed_channel_intensity ** j)\n",
    "    #\n",
    "    # s = 0\n",
    "    # for j in range(channel_count + 1, channel_count + waiting_queue_limit + 1):\n",
    "    #     s += request_intensity ** j / (np.math.factorial(channel_count) * channel_count ** (j - channel_count) * changed_channel_intensity ** j)\n",
    "    #\n",
    "    # return theor / (f + s)\n",
    "\n",
    "\n",
    "def make_bandwidth_statistic(pract_finish_state, theor_finish_state, to_print):\n",
    "    print('')\n",
    "\n",
    "    pract_relative_bandwidth = 1 - pract_finish_state[-1]\n",
    "    theor_relative_bandwidth = 1 - theor_finish_state[-1]\n",
    "    pract_absolute_bandwidth = request_intensity * pract_relative_bandwidth\n",
    "    theor_absolute_bandwidth = request_intensity * theor_relative_bandwidth\n",
    "\n",
    "    print('Relative bandwidth: {} - {}, {}'.format(pract_relative_bandwidth, theor_relative_bandwidth, abs(pract_relative_bandwidth - theor_relative_bandwidth)))\n",
    "    print('Absolute bandwidth: {} - {}, {}'.format(pract_absolute_bandwidth, theor_absolute_bandwidth, abs(pract_absolute_bandwidth - theor_absolute_bandwidth)))\n",
    "\n",
    "\n",
    "def make_cancellation_statistic():\n",
    "    print('')\n",
    "\n",
    "    pract_cancellations = all_cancelled / all_requests\n",
    "    theor_cancellations = theor_state_prob[-1] + theor_state_prob[-2]*(1 - accept_prob)\n",
    "\n",
    "    print(\"Cancellations: {} - {}, {}\".format(pract_cancellations, theor_cancellations, abs(pract_cancellations - theor_cancellations)))\n",
    "\n",
    "\n",
    "def make_average_waiting_queue_length(to_print):\n",
    "    pract_average = stat.mean(waiting_queue_len_in_time)\n",
    "    theor_average = 0\n",
    "    for i in range(1, waiting_queue_limit + 1):\n",
    "        theor_average += i * theor_state_prob[channel_count + i]\n",
    "\n",
    "    if to_print:\n",
    "        print('')\n",
    "        print(\"Average waiting queue length: {} - {}, {}\".format(pract_average, theor_average, abs(pract_average - theor_average)))\n",
    "    return theor_average, pract_average\n",
    "\n",
    "def make_average_working_channels_count(to_print):\n",
    "    pract_average = stat.mean(working_channels_count_in_time)\n",
    "    theor_average = 0\n",
    "    for i in range(channel_count + 1):\n",
    "        theor_average += i * theor_state_prob[i]\n",
    "    for i in range(1, waiting_queue_limit + 1):\n",
    "        theor_average += channel_count * theor_state_prob[channel_count + i]\n",
    "\n",
    "    if to_print:\n",
    "        print('')\n",
    "        print(\"Average working channels count: {} - {}, {}\".format(pract_average, theor_average, abs(pract_average - theor_average)))\n",
    "    return theor_average\n",
    "\n",
    "def make_average_requests_in_system_count(to_print, average_waiting, average_channels):\n",
    "    pract_average = stat.mean(requests_count_in_system_in_time)\n",
    "    theor_average = average_waiting + average_channels\n",
    "\n",
    "    if to_print:\n",
    "        print('')\n",
    "        print(\"Average requests count in system: {} - {}, {}\".format(pract_average, theor_average, abs(pract_average - theor_average)))\n",
    "    return theor_average, pract_average\n",
    "\n",
    "def make_average_request_live_in_system(to_print, average_request_system_count, average_request_system_count_pract):\n",
    "    request_live_diff = []\n",
    "    for val, start in request_live_start.items():\n",
    "        request_live_diff.append(request_live_end[val] - start)\n",
    "\n",
    "    pract_average = average_request_system_count_pract / request_intensity\n",
    "    theor_average = average_request_system_count / (request_intensity)\n",
    "\n",
    "    if to_print:\n",
    "        print('')\n",
    "        print(\"Average requests live time in system: {} - {}, {}\".format(pract_average, theor_average,\n",
    "                                                                 abs(pract_average - theor_average)))\n",
    "\n",
    "    return pract_average\n",
    "\n",
    "\n",
    "def make_average_request_live_in_queue(average_request_queue_count, average_request_queue_count_pract):\n",
    "    print('')\n",
    "\n",
    "    request_live_diff = []\n",
    "    for val, start in request_in_queue_start.items():\n",
    "        if request_in_queue_end.get(val, None) is not None:\n",
    "            request_live_diff.append(request_in_queue_end[val] - start)\n",
    "\n",
    "    pract_average = average_request_queue_count_pract / (request_intensity)\n",
    "    theor_average = average_request_queue_count / (request_intensity)\n",
    "\n",
    "    print(\"Average requests live time in queue: {} - {}, {}\".format(pract_average, theor_average,\n",
    "                                                                 abs(pract_average - theor_average)))\n",
    "\n",
    "\n",
    "def make_average_uniqe_requests(average_request_system_count):\n",
    "    print('')\n",
    "\n",
    "    pract_average = stat.mean(unique_requests_in_time)\n",
    "    theor_average = average_request_system_count * accept_prob\n",
    "\n",
    "    print(\"Average uniqe requests in system: {} - {}, {}\".format(pract_average, theor_average,\n",
    "                                                                 abs(pract_average - theor_average)))\n",
    "\n",
    "\n",
    "def reset():\n",
    "    global waiting_queue, channels, all_accepted, all_rejected, all_done, all_cancelled, current_loop, next_message_loop\n",
    "    global finish_state_count, finish_state_prob_in_time, requests_count_in_system_in_time, waiting_queue_len_in_time\n",
    "    global working_channels_count_in_time, request_live_start, request_live_end, request_in_queue_start\n",
    "    global request_in_queue_end, rejected_count, unique_requests_in_time, accepted_count_in_time\n",
    "\n",
    "    waiting_queue = []\n",
    "    channels = []\n",
    "    all_accepted = 0\n",
    "    all_rejected = 0\n",
    "    all_done = 0\n",
    "    all_cancelled = 0\n",
    "    current_loop = 0\n",
    "    next_message_loop = -1\n",
    "\n",
    "    finish_state_count = []\n",
    "    finish_state_prob_in_time = []\n",
    "    requests_count_in_system_in_time = []\n",
    "    waiting_queue_len_in_time = []\n",
    "    working_channels_count_in_time = []\n",
    "    request_live_start = {}\n",
    "    request_live_end = {}\n",
    "    request_in_queue_start = {}\n",
    "    request_in_queue_end = {}\n",
    "    rejected_count = {}\n",
    "    unique_requests_in_time = []\n",
    "    accepted_count_in_time = []\n",
    "\n",
    "def run(loop_limit, to_print=True, finish_state_statistic_only=False, system_live_time_only=False):\n",
    "    global channels, current_loop, next_message_loop\n",
    "\n",
    "    reset()\n",
    "\n",
    "    init_statistic()\n",
    "    channels = start_channels()\n",
    "\n",
    "    if to_print:\n",
    "        print_system_state()\n",
    "\n",
    "    diff = generate_next_exponential(request_intensity)\n",
    "    next_message_loop = current_loop + diff\n",
    "\n",
    "    while True:\n",
    "        current_loop += 1\n",
    "        if current_loop == loop_limit:\n",
    "            break\n",
    "\n",
    "        refresh_channels()\n",
    "\n",
    "        take_statistic()\n",
    "\n",
    "        try_create_new_request()\n",
    "\n",
    "        #print_system_state()\n",
    "        #time.sleep(1)\n",
    "\n",
    "\n",
    "    for val in waiting_queue:\n",
    "        request_live_end[val] = current_loop\n",
    "        request_in_queue_end[val] = current_loop + request_in_queue_end.get(val, 0)\n",
    "\n",
    "    for channel in channels:\n",
    "        if channel.working:\n",
    "            request_live_end[channel.value] = current_loop\n",
    "\n",
    "    if finish_state_statistic_only:\n",
    "        return make_finish_state_statistic(to_print)\n",
    "\n",
    "    if system_live_time_only:\n",
    "        average_waiting = make_average_waiting_queue_length(to_print)\n",
    "        average_channels = make_average_working_channels_count(to_print)\n",
    "        average_system, average_system_pract = make_average_requests_in_system_count(to_print, average_waiting, average_channels)\n",
    "        return make_average_request_live_in_system(to_print, average_system, average_system_pract)\n",
    "\n",
    "    pract_finish_state, theor_finish_state, max_diff = make_finish_state_statistic(to_print)\n",
    "    make_bandwidth_statistic(pract_finish_state, theor_finish_state, to_print)\n",
    "    make_cancellation_statistic()\n",
    "    average_waiting, average_waiting_pract = make_average_waiting_queue_length(to_print)\n",
    "    average_channels = make_average_working_channels_count(to_print)\n",
    "    average_system, average_system_pract = make_average_requests_in_system_count(to_print, average_waiting, average_channels)\n",
    "    make_average_request_live_in_system(to_print, average_system, average_system_pract)\n",
    "    make_average_request_live_in_queue(average_waiting, average_waiting_pract)\n",
    "\n",
    "\n",
    "def run_max_diff_finish_state_test():\n",
    "    x_array = range(100)\n",
    "    y_array = []\n",
    "    for x in x_array:\n",
    "        y_array.append(run(loop_limit=10000, to_print=False, finish_state_statistic_only=True)[2])\n",
    "\n",
    "    plt.plot(x_array, y_array)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_live_time_test():\n",
    "    x_array = range(10000, 150000, 10000)\n",
    "    y_array = []\n",
    "    for x in x_array:\n",
    "        print('Done', x)\n",
    "        y_array.append(run(loop_limit=x, to_print=False, system_live_time_only=True))\n",
    "\n",
    "    plt.plot(x_array, y_array)\n",
    "    plt.show()\n",
    "\n",
    "def show_stationary(states = None):\n",
    "    x = range(len(finish_state_prob_in_time))\n",
    "    if states is None:\n",
    "        states = range(len(finish_state_prob_in_time[0]))\n",
    "    for state in states:\n",
    "        y = []\n",
    "        ty = []\n",
    "        for t in range(len(finish_state_prob_in_time)):\n",
    "            y.append(finish_state_prob_in_time[t][state])\n",
    "            ty.append(theor_state_prob[state])\n",
    "        plt.plot(x, y)\n",
    "        plt.plot(x, ty)\n",
    "    plt.show()\n",
    "\n",
    "def run_max_diff_finish_state_to_loop_limit_test():\n",
    "    x_array = range(100, 1000, 100)\n",
    "    y_array = []\n",
    "    for x in x_array:\n",
    "        test = []\n",
    "        for i in range(100):\n",
    "            test.append(run(loop_limit=x, to_print=False, finish_state_statistic_only=True)[2])\n",
    "        y_array.append(stat.mean(test))\n",
    "\n",
    "    plt.plot(x_array, y_array)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def solve_theor_states_prob(with_addition=True):\n",
    "    symbols = [sp.Symbol('p' + str(i)) for i in range(channel_count + waiting_queue_limit + 1)]\n",
    "\n",
    "    system = []\n",
    "    system.append(-request_intensity * symbols[0] + accept_prob * channel_intensity * symbols[1])\n",
    "    for i in range(1, channel_count):\n",
    "        system.append(\n",
    "            request_intensity * symbols[i - 1] - (request_intensity + i * accept_prob * channel_intensity) * symbols[\n",
    "                i] + (i + 1) * accept_prob * channel_intensity * symbols[i + 1])\n",
    "    for i in range(channel_count, channel_count + waiting_queue_limit - 1):\n",
    "        system.append(\n",
    "            request_intensity * symbols[i - 1] - (request_intensity + channel_count * accept_prob * channel_intensity) *\n",
    "            symbols[i] + channel_count * accept_prob * channel_intensity * symbols[i + 1])\n",
    "\n",
    "    system.append(\n",
    "        request_intensity * symbols[-3] - (request_intensity + channel_count * accept_prob * channel_intensity) *\n",
    "        symbols[-2] + channel_count * channel_intensity * symbols[-1])\n",
    "\n",
    "    if with_addition:\n",
    "        system.append(request_intensity * symbols[-2] - channel_count * channel_intensity * symbols[-1])\n",
    "\n",
    "    last = symbols[0]\n",
    "    for i in range(1, channel_count + waiting_queue_limit + 1):\n",
    "        last += symbols[i]\n",
    "    last -= 1\n",
    "    system.append(last)\n",
    "\n",
    "    print(system)\n",
    "\n",
    "    res = list(linsolve(system, *symbols))\n",
    "    if len(res) == 0 and with_addition:\n",
    "        return solve_theor_states_prob(False)\n",
    "    return list(res[0])\n",
    "\n",
    "\n",
    "def calculate_theor_states_prob():\n",
    "    theor_state_prob = [0] * (channel_count + waiting_queue_limit + 1)\n",
    "\n",
    "    if accept_prob == 0:\n",
    "        theor_state_prob[-2] = channel_count * channel_intensity / (channel_count * channel_intensity + request_intensity)\n",
    "        theor_state_prob[-1] = 1 - theor_state_prob[-2]\n",
    "    else:\n",
    "        first_sum = 0\n",
    "        for i in range(channel_count + 1):\n",
    "            first_sum += request_intensity ** i / (np.math.factorial(i) * (channel_intensity * accept_prob) ** i)\n",
    "        second_sum = 0\n",
    "        for i in range(channel_count + 1, channel_count + waiting_queue_limit):\n",
    "            second_sum += request_intensity ** i / (np.math.factorial(channel_count) * channel_count ** (i - channel_count) * (channel_intensity * accept_prob) ** i)\n",
    "        res_sum = first_sum + second_sum + request_intensity ** (channel_count + waiting_queue_limit) / (np.math.factorial(channel_count) * channel_count ** waiting_queue_limit * channel_intensity ** (channel_count + waiting_queue_limit))\n",
    "\n",
    "        theor_state_prob[0] = 1 / res_sum\n",
    "        for i in range(1, channel_count + 1):\n",
    "            theor_state_prob[i] = request_intensity ** i / (np.math.factorial(i) * (channel_intensity * accept_prob) ** i) * theor_state_prob[0]\n",
    "\n",
    "        for i in range(channel_count + 1, channel_count + waiting_queue_limit):\n",
    "            theor_state_prob[i] = request_intensity ** i / (np.math.factorial(channel_count) * channel_count ** (i - channel_count) * (request_intensity * accept_prob) ** i) * theor_state_prob[0]\n",
    "\n",
    "        theor_state_prob[channel_count + waiting_queue_limit] = request_intensity ** (channel_count + waiting_queue_limit) / (np.math.factorial(channel_count) * channel_count ** waiting_queue_limit * request_intensity ** (channel_count + waiting_queue_limit) * accept_prob ** (channel_count + waiting_queue_limit - 1)) * theor_state_prob[0]\n",
    "\n",
    "    return theor_state_prob\n",
    "\n",
    "request_intensity = 0.01\n",
    "channel_intensity = 0.005\n",
    "channel_count = 2\n",
    "waiting_queue_limit = 3\n",
    "accept_prob = 0.3\n",
    "\n",
    "theor_state_prob = calculate_theor_states_prob()\n",
    "run(10000)\n",
    "\n",
    "plt.plot(range(len(accepted_count_in_time)), accepted_count_in_time)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "run_live_time_test()\n",
    "\n",
    "show_stationary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
